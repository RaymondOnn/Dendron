---
id: ixx70tg3sfp7ybbddnxok6v
title: 04_KStreamsCoreConcepts
desc: ''
updated: 1700013876165
created: 1699880422664
---
Welcome back to Kafka Big picture for beginners. 
In the earlier lecture, I talked about the five core components of Kafka ecosystem and already covered the first three. 
These three items are to help you create a simplified and manageable data integration solution. 
So, if all that you wanted to do is to integrate data among your applications, then you should stop learning Kafka after you learn the first three items. 
The next two things will take your Kafka implementation beyond the data integration and allow you to create a scalable and fault tolerant real time stream processing application. 
In this lecture, I'll be explaining the core concepts of Kafka streams. 
This lecture will give you a definite answer to the following questions. 
Let's start with the first question. 
What is real time stream processing? I guess you already have some idea about it. 
But to cement the idea in your brain, let me quickly highlight a few things. 
The first and the most important thing is to understand that the data streams are an unbounded, infinite and ever growing sequence of data that is continuously generated and sent in small sizes in order of KBs. 
Here are some examples. 
Sensors - sending data from transportation vehicles, industrial equipment, healthcare devices, and wearables. 
Log Entries - generated by mobile apps, web applications, infrastructure components. 
Click Streams - generated by e-commerce applications, news, and media websites, video streaming, online gaming. 
Transactions - coming from the Stock market, Credit Card, ATM machines, eCommerce orders, payments, logistics, and food deliveries. 
Feed - for example social media activities, traffic activities, security and threat systems. 
The list is endless because if you look around, you are going to realize that pretty much everything can be seen as a sequence of smart data packets. 
And these are unbounded means infinite and ever growing. 
Once started, they never ever end. 
Now we have the challenge to process them. 
One common approach is to collect and store them in a storage system. 
Then you can do two things. 
Query the data to get answers to a specific question. 
This is what we know as a request-response approach and usually handled through a SQLs. 
This approach is all about asking one question at a time and getting the answer to the question as quickly as possible. 
The second approach is to create one big job to find answers to a bunch of queries and schedule the job to run at regular intervals. 
This approach is all about asking a bunch of questions at once and repeating the question every hour or maybe every day. 
And this approach is known as batch processing. 
The stream processing sits in-between these two approaches. 
In this approach, we ask a question once, and the system should give you the most recent version of the answer all the time. 
So, stream processing is a continuous process and the business reports are updated continuously based on the data available till time. 
As we receive more data, the reports are updated and refreshed with the new information. 
However, under the hood, you might be doing the same usual operations as joining two streams, grouping your data, computing aggregates, and other ordinary things that we do in database queries and batch processing systems. 
After all, stream processing is also a data processing work. 
Isn't it? But we do it in a continuous and ongoing process. 
Now the question is this - Can we use databases or batch processing systems to perform stream processing? The answer is yes and no. 
Technically it is possible to use databases and bad systems to do a stream processing. 
So the answer is yes. 
However dealing with data in realtime using those systems is going to make your solution too complex. 
Also there are some new concepts and considerations which are going to make your work more difficult. 
So we need a tool that is specificly designed for stream processing. 
And Kafka streams is exactly what you might be looking for. 
You might be wondering about this question. 
Can we do it using Kafka producers, consumers, and Kafka Connect? Or are we going to need Kafka streams? So the point is this - real-time stream processing is not as straightforward as data integration. 
It poses some new challenges, and you cannot solve those problems using tools that were designed to address deep data integration problem. 
I mean, Kafka producer, consumer, and Kafka connect. 
They are not an answer for stream processing. 
You need a new tool that is designed and developed to meet your stream processing requirements, and that's what Kafka streams is all about. 
Now the next question - What is Kafka streams and how it solves is stream processing challenges? At the most basic level, Kafka Streams is a library for building applications and microservices where the input data are streamed in a Kafka topic. 
So you cannot use Kafka streams if your data is not coming to a Kafka topic. 
The starting point for the Kafka Stream is one or more Kafka topics. 
Right? Now the most powerful feature of Kafka streams is being a simple library. 
So, you can create a standard Java and Scala applications to perform real time stream processing. 
And you can deploy your applications to any machine, virtual machine, container, or on a Kubernetes cluster. 
Your application is just another typical application with inherent parallel processing capability, fault tolerance and scalability which is given to you by the Kafka streams library as an out of the box capability. 
Other than this, the Kafka streams libraries is specifically designed for the sole purpose of stream processing. 
So, it allows you to handle all those unique streaming challenges. 
Here is a list of the most critical capabilities. 
Working with streams tables, and interoperating with them. 
I mean, you can mix and match your solutions with streams and tables and you can even convert a stream to a table and vice versa. 
It allows you to group your streams and compute continuously updating aggregates. 
You can join streams, tables, and a combination of both. 
You can create and manage fault tolerant, efficient local state stores. 
Creating windows of different types and dealing with all the time domain complexities such as even time, processing time, latecomers, high watermark, exactly ones processing etc. 
It also allows you to serve other microservices using request/response interface over and above your streams application. 
This feature is also known as Kafka streams interactive query. 
It gives you a set of tools for unit testing your application. 
The library gives you an easy to use DLS and also offers flexibility to extend and create your custom processors beyond what is provided. 
You can get an inherent fault tolerance and dynamic scalability. 
You can deploy your stream processing applications in containers and manage them in the Kubernetes Cluster. 
And this list is not exhaustive. 
Now the concluding question - How it works? Kafka streams architecture. 
So the Kafka streams is all about continuously reading a stream of data from one or more Kafka topics. 
And then, you develop your application logic to process those streams in real-time and take necessary actions. 
So assume that you created a Kafka streams application and deployed it on a single machine, right? Your application will be running here. 
Now imagine that your Kafka streams application is continuously consuming data from two topics, T1 and T2 with each having three partitions. 
I'm not going into the functionality of your application. 
It might be monitoring traffic data or maybe patient vitals, continuously checking some thresholds, and sending alerts when the threshold breaks, right? We are trying to understand the scalability and fault tolerance of your simple and tiny application. 
So you deploy your application on a single machine. 
Now the Kafka streams will internally create three logical tasks because the maximum number of partitions across the input topics T1 and T2 are three. 
So the Kafka stream's framework knows that we can create three consumers where each could be consuming from one partition in parallel. 
The most exciting part is you don't have to code this thing. 
The framework is smartly detects it and creates three logical tasks. 
The next step is to assign partitions to these tasks. 
In this case, the Kafka framework would allocate the partitions evenly. 
That is one partition from each topic to each task. 
At the end of this assignment, every task will have two partitions to process. 
Now, these tasks are ready to be assigned to application threads or instances. 
If you configured the application to run with two threads, Kafka would assign one task to each thread. 
And the remaining third task will go to one of these threads because we do not have any other thread. 
In this case, the task distribution is not even. 
The thread running two task might run slow. 
However, all tasks would be running and ultimately all the data gets processed. 
You might be wondering why I said to configure the application to run with two threads? Yes, you heard it right. 
You can make your coffee streams become a multi threaded application by simply setting the number of Max threads. 
It is that simple. 
So as of now you will have one application instance running on a single machine and sharing the workload in two parallel threads. 
Now imagine we want to scale out this application. 
We decided to start another instance with a single thread on a different machine. 
A new thread T3 will be created, and one task would automatically migrate to the new third. 
This happens automatically and known as Task re-assignment. 
When the task reassignment occurs, task partitions and their corresponding local estate stores will also migrate from the existing threat to the newly added thread. 
As a result, Kafka streams has effectively rebalanced the work load among instances of the application at the granularity of Kafka topic partitions. 
All this happens automatically and without stopping or restarting your application. 
What if we wanted to add even more instances of the same application? Well you can do that, but they will be doing nothing. 
Why? Because we have only three tasks, which is equal to the number of available input partitions to read from. 
Adding more instances beyond that point is an over provisioning with idle instances. 
What about fault tolerance? What happens if an active instance dies or goes down? Kafka streams is out of the box for tolerance. 
If a tasker runs on a machine that fails, Kafka streams automatically restarts the task in one of the remaining running instances. 
As a result, failure handling is entirely transparent to the end-user, and it is taken care of by the framework itself. 
That's all about Kafka streams. 
See you in the next lecture. 
Keep learning and keep growing.