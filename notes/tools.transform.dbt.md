---
id: 9u507bpmqqfb71396hy112n
title: dbt
desc: ''
updated: 1717579441645
created: 1691456598508
---

dbt mesh
dbt project dependencies
<https://www.youtube.com/watch?v=NQmOiEJ8fEs>

> ### Common DBT commands
>
> - `dbt debug`: Runs a dry-run of a dbt command without actually executing the command.
> - `dbt parse`: parses and validates the contents of the project.
> - `dbt compile`: Compiles the SQL in your dbt project, generating the final SQL code that will be executed against your data warehouse, stored in `target` directory
> --select a specific node by name
>   - `--inline <dbt-SQL_QUERY>`: display the compiled code of the dbt-SQL query:
>   - `--select <MODEL>`: display the compiled code of a node
>   - `--no-populate-cache`: dbt flage to disable the initial cache population.
>   - `--no-introspect`: `dbt compile` flag to disable introspective queries i.e. compilation requires running SQL queries/
> - `dbt build`: `dbt run` + `dbt test`
>   - `dbt run`: Executes the compiled SQL in your data warehouse.
>   - `dbt test`: Runs tests defined in your dbt project, checking for errors or inconsistencies in your data.
> - `dbt deps`: Installs dependencies for your dbt project.
> - `dbt docs generate`: Generates documentation for your dbt project.
> - `dbt docs serve`: Serves the documentation generated by dbt docs generate on a local server.
> - `dbt seed`: Seeds your data warehouse with initial data.
> - `dbt snapshot`: Takes a snapshot of your data warehouse, capturing the current state of your data.
> - `dbt snapshot-freshness`: Checks the freshness of your snapshots and generates a report indicating which snapshots need to be refreshed.
> - `dbt run-operation`: Call macro and then exit
>   -‍ `--args <ARGS_DICT>`: Supply arguments to the macro eg. '{my_variable: my_value}'

---

---

### What is dbt

- an open-source command-line tool that helps organizations build, test, and maintain their data infrastructure.
- it provides a consistent and standardized approach to data transformation and analysis.
- A framework to organise SQL files and not a scheduler that will be able out of the box run your transformation on a fixed schedule.
- Note that there is not any kind of processing within dbt

### How it works

dbt allows users to define their data models using SQL, and then uses these models to generate optimized SQL code that can be run against a data warehouse or other data storage system. This allows users to build a maintainable and scalable data infrastructure that can be easily updated and extended over time.

.

### Getting Started

- **a dbt project**:
  - a dbt project is a folder that contains all the dbt objects needed to work.
  - You can initialise a project with the CLI command: `dbt init`.
- **`profiles.yml`**:
  - This file contains the credentials to connect your dbt project to your data warehouse.
  - By default this file is located in your `$HOME/.dbt/` folder.
  - Recommended to create your own profiles file and to specify the `--profiles-dir` option to the dbt CLI.
  - A connection to a warehouse requires a dbt adapter to be installed.

### dbt Entities

[[tools.transform.dbt.models]]

#### Jinja

##### `ref` / `source` functions

- When writing a model, `ref` and `source` macros are used to define the relationships between models

  - `source`: Indicates a relation to source i.e. the raw data that is loaded into the data warehouse.
    - See [[tools.transform.dbt.models.sources]] for more info.

        ```sql
        -- references to the YAML file created
        ...
        FROM {{ source ('jaffle_shop', 'customers') }}
        ...
        ```

  - `ref`: define a relation to another model, or other kind of dbt resources.

        ```sql
        -- input name of model as param
        WITH customers as (
            SELECT *
            FROM {{ ref ('stg-customer') }}
        )
        ...
        ```

- dbt uses this information to create a lineage graph between the models.

##### Macros

- To make sql queries dynamic, dbt leverage the use of jinja macros. See [[tools.templating.jinja]].
- Macros are functions that are written in Jinja
- Macros allows us to write generic logic once and reuse that logic throughout the project
- Macros are defined in `.sql` files, typically in your `macros` directory.
- Macro can be imported from other dbt packages or defined within a dbt project.

> ###### NOTE: DRY vs Readable Code
>
> - Consider the tradeoff between abstracting the code away over clarity and ease of writing the code
> - Important to balance the readability/maintainability of the code with how concise the code (or DRY) the code is.

```sql
--macros/left4.sql
{% macro left4(column_name) %}
    LEFT({{ column_name }}, 4)
{% endmacro %}

--models/test_model.sql
-- Using jinja macro in SQL query
SELECT
    {{ left4("column1") }} AS column1,
    {{ left4("column2") }} AS column2,
    {{ left4("column3") }} AS column3
FROM
    source_table
```

#### Tests

- dbt has a testing framework that can be used to check data quality.
- Testing ensures that any changes or updates to the data pipeline do not introduce
- Tests are written as `SELECT` statements. These select statements are run against your materialized models to ensure they meet your assertions.errors or inconsistencies.
- Additional testing can be imported through packages

##### Generic Tests

- Generic tests are configured in YAML
- These are run on specific columns in a model and return the number of records that do not meet your assertions.
- dbt ships with four generic tests already defined:
  - unique
  - not_null
  - relationships.(i.e. values exists in column of another table)

        ```yaml
        models:
            - name: stg_orders
                columns:
                    - name: customer_id
                        tests:
                            - relationships:
                                to: ref('stg_customers')
                                field: customer_id
        ```

  - accepted_values (i.e. values from list of values)

```yaml
# models/staging/stg_jaffle_shop.yml
version: 2

models:
    - name: stg_customers
      columns:
          - name: customer_id
            tests:
                - unique
                - not_null

    - name: stg_orders
      columns:
          - name: order_id
            tests:
                - unique
                - not_null
          - name: status
            tests:
                - accepted_values:
                      values:
                          - completed
                          - shipped
                          - returned
                          - return_pending
                          - placed
```

##### Singular Tests

- Singular tests are specific queries that you run against your models. These are run on the entire model.
- Stored as `SELECT` statements in the `tests` folder.
- Note how this is a `.sql` file in the `tests` directory
- `ref` / `source` function used to associate test to a model / source

```sql
-- tests/assert_positive_total_for_payments.sql
-- Refunds have a negative amount, so the total amount should always be >= 0.
-- Therefore return records where this isn't true to make the test fail.
select
    order_id,
    sum(amount) as total_amount
from {{ ref('stg_payments') }}   -- associates this test with stg_payments
group by 1
having not(total_amount >= 0)
```

##### Testing Sources

- Define the tests in the same `.yml` file where the sources were defined

```sql
-- models/staging/jaffle_shop/src_jaffle_shop.yml

version: 2

sources:
  - name: jaffle_shop
    database: raw
    schema: jaffle_shop
    tables:
      - name: customers
        columns:
          - name: id
            tests:          -- added tests
              - unique
              - not_null

      - name: orders
        columns:
          - name: id
            tests:          -- added tests
              - unique
              - not_null
        loaded_at_field: _etl_loaded_at
        freshness:
          warn_after: {count: 12, period: hour}
          error_after: {count: 24, period: hour}
```

- `dbt test --select source:<SOURCE_NAME>`: Run tests for source for e.g. `dbt test --select source:jaffle_shop`

##### Running Tests

- `dbt test` to run all generic and singular tests in your project.
  - When executed, some SQL wrapper code is generated

        ```sql
        SELECT
            COUNT(*) as failures,
            COUNT(*) != 0 as should_warn,
            COUNT(*) != 0 as should_error
        FROM (
            <GENERIC_TEST_SQL>
        )
        ```

- `dbt test --select test_type:generic` to run only generic tests in your project.
- `dbt test --select test_type:singular` to run only singular tests in your project.
- `dbt test --select one_specific_model`
- you can mention in the schema.yml files under the DBT projects and custom tests that you can define in the tests directory as a .sql file.
- To store the failed results as table, use `dbt test — store-failures`

    ```yaml
    -- models/schema.yml
    version: 2
    #Include here all source models.

    version: 2

    seeds:

    -   name: STUDENT
        description: student raw data
        columns:
        -   name: ID
            description: ID of the student
            data_type: INTEGER
            tests:
            -   unique
            -   not_null
        -   name: NAME
            description: NAME of the student
            data_type: string
    ```

    ```sql
    -- models/my_model.sql
    -- checks the number of records in my_model where column 1 is greater than 10.
    SELECT
        column1,
        column2,
        column3
    FROM source_table
    WHERE column1 > 10

    -- tests/my_model_test.sql
    SELECT COUNT(\*) AS record_count
    FROM {{ source('my_model') }}
    WHERE column1 > 10
    ```

#### Docs

- dbt offers robust documentation capabilities, making it easier to maintain a central repository of metadata, schema definitions, and data lineage.
- Documentation helps data teams understand the context and purpose of each model, facilitating knowledge sharing.
- Documentation of models occurs in the YML files (where generic tests also live) inside the models directory. It is helpful to store the YML file in the same subfolder as the models you are documenting.

##### Documenting Models

- One way to add info is through text via `description` field
- For models, descriptions can happen at the model, source, or column level.
- A another method is via `doc blocks` i.e. `'{{ doc("order_status") }}'`

  - used to render markdown in the generated documentation
  - When referencing the `doc block`, use the name of the `doc block`, not the markdown filename
  - Allows us to store multiple `doc blocks` in one single file

        ```md
        models/staging/jaffle_shop/jaffle_shop.md

        {% docs order_status %} <-- name of doc block

        One of the following values:

        | status         | definition                                       |
        | -------------- | ------------------------------------------------ |
        | placed         | Order placed, not yet shipped                    |
        | shipped        | Order has been shipped, not yet been delivered   |
        | completed      | Order has been received by customers             |
        | return pending | Customer indicated they want to return this item |
        | returned       | Item has been returned                           |

        {% enddocs %} <-- end of doc block
        ```

```yaml
# models/staging/jaffle_shop/stg_jaffle_shop.yml

version: 2

models:
    - name: stg_customers
      description: Staged customer data from our jaffle shop app.
      columns:
          - name: customer_id
            description: The primary key for customers.
            tests:
                - unique
                - not_null

    - name: stg_orders
      description: Staged order data from our jaffle shop app.
      columns:
          - name: order_id
            description: Primary key for orders.
            tests:
                - unique
                - not_null
          - name: status
            description: '{{ doc("order_status") }}' # add doc block
            tests:
                - accepted_values:
                      values:
                          - completed
                          - shipped
                          - returned
                          - placed
                          - return_pending
          - name: customer_id
            description: Foreign key to stg_customers.customer_id.
            tests:
                - relationships:
                      to: ref('stg_customers')
                      field: customer_id
```

##### Documenting Sources

- Documenting is the same as for documenting models

```yaml
# models/staging/jaffle_shop/src_jaffle_shop.yml

version: 2

sources:
    - name: jaffle_shop
      description: A clone of a Postgres application database.
      database: raw
      schema: jaffle_shop
      tables:
          - name: customers
            description: Raw customers data.
            columns:
                - name: id
                  description: Primary key for customers.
                  tests:
                      - unique
                      - not_null

          - name: orders
            description: Raw orders data.
            columns:
                - name: id
                  description: Primary key for orders.
                  tests:
                      - unique
                      - not_null
            loaded_at_field: _etl_loaded_at
            freshness:
                warn_after: { count: 12, period: hour }
                error_after: { count: 24, period: hour }
```

##### Generating Docs

- `dbt docs generate`: generate all files for hosting dbt documentation website

  - To reflect new chamges. re-generating the docs is required

    ```sh
    apareek@CA-1197 dwh_ayush % dbt docs generate
    14:27:19 Running with dbt=1.2.6
    14:27:19 Found 6 models, 4 tests, 1 snapshot, 0 analyses, 269 macros, 0 operations, 5 seed files, 0 sources, 0 exposures, 0 metrics
    14:27:19
    14:27:23 Concurrency: 1 threads (target='dev')
    14:27:23
    14:27:23 Done.
    14:27:23 Building catalog
    14:27:27 Catalog written to /Users/apareek/Documents/dbt_poc/dwh_ayush/target/catalog.json
    ```

- `dbt docs serve` is used to spin up a local http server and host the dbt documentation site.

    ```sh
    apareek@CA-1197 dwh_ayush % dbt docs serve --port 8110
    14:33:34 Running with dbt=1.2.6
    14:33:34 Serving docs at 0.0.0.0:8110
    14:33:34 To access from your browser, navigate to: http://localhost:8110
    14:33:34
    14:33:34
    14:33:34 Press Ctrl+C to exit.
    127.0.0.1 - - [06/Aug/2023 16:33:35] "GET / HTTP/1.1" 200 -
    127.0.0.1 - - [06/Aug/2023 16:33:35] "GET /manifest.json?cb=1691332415139 HTTP/1.1" 200 -
    127.0.0.1 - - [06/Aug/2023 16:33:35] "GET /catalog.json?cb=1691332415139 HTTP/1.1" 200 -
    ```

- There is a data lineage graph available for a better understanding of how data is flowing in the transformation.

### Incremental Data Processing

DBT supports incremental data processing, which is crucial for managing large datasets efficiently. Rather than reprocessing the entire dataset, incremental models in DBT only process the new or updated records. This approach significantly reduces processing time, making it feasible to perform frequent data updates without incurring excessive overhead. Incremental models also contribute to cost optimization by minimizing the resources required for processing and storage.

1. Incremental load
   There are a lot of ways to run the model in incremental mode. In simple words only process the latest data.
   — Directly using Jinja template if condition as below.

``` sql
-- models/my_model.sql
{{
  config(
    materialized='incremental'
  )
}}

SELECT \*,
{{ var("raisin_source_id") }} AS DWH_SOURCE_ID,
{{ var("dwh_loaddatetime") }} as DWH_LOADDATETIME,
{{ var("dwh_loaddate") }} as DWH_LOADDATE
FROM {{ ref('SUBJECT') }}
{% if is_incremental() %}
where
ID > (
select coalesce(max(ID), '0') from {{ this }}
)
{% endif %}
```

— Another option is by specifying a unique id/incremental strategy in the config block of the model.

``` sql
{{
  config(
    materialized='incremental',
    incremental_strategy='merge', --Other options append, delete+insert
    unique_key='STU_ID'
  )
}}

SELECT STU_ID,
STUDENT_NAME,
SUM(MARKS) TOTAL_MARKS,
{{ var("dwh_loaddatetime") }} as DWH_LOADDATETIME,
{{ var("dwh_loaddate") }} as DWH_LOADDATE
FROM {{ ref('STUDENT_SUMMARY') }}
GROUP BY STU_ID,
STUDENT_NAME 
```

7. Snapshot
Snapshots implement type-2 Slowly Changing Dimensions over mutable source tables. These Slowly Changing Dimensions (or SCDs) identify how a row in a table changes over time but the main part of DBT snapshot is you just need to write a simple SELECT and the rest is taken care of by DBT itself.

let’s take a well-known example of an address table.

``` sql
--snapshots/address_snapshot.sql
{% snapshot address_snapshot %}

{{
config(
unique_key='NAME',
target_schema='summary',

      strategy='timestamp',
      updated_at='valid_from',
    )

}}

select NAME,
CITY,
VALID_FROM
from {{ ref('ADDRESS') }}

{% endsnapshot %}
```

Now run the above DBT snapshot dbt snapshot — select ADDRESS_SNAPSHOT and check the output. We can see DBT automatically added the column required to same the history.
